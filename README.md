# Question_Generation
This study focus on generating grammatically correct,meaningful and solvable questions using
machine learning models utilizing transformers, an attention-based model that rejects the
concept of pre-existing recurrent neural networks(RNN). Four models using T5 transformers are
implemented using Wordpiece and Sentencepiece tokenizer and their performance is compared.
Word Error Rate (WER) and Bidirectional Encoder Representations from Transformers (BERT)
scores are utilised as a criterion to assess how closely SQuAD questions and model-generated
questions resemble one another.

WER Scores

![alt text]([http://url/to/img.png](https://github.com/Narayanan-ramesh/Question_Generation/blob/main/Images/WER.PNG))


Comparison of 4 models


![alt text]([http://url/to/img.png](https://github.com/Narayanan-ramesh/Question_Generation/blob/main/Images/Scores.PNG))
